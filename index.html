<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Mathematical Perspective on Transformers</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>A Mathematical Perspective on Transformers</h1>
        <h2>The emergence of clusters in self-attention dynamics</h2>
        <p>By Josef Ott</p>
        <p>TUM</p>
        <p>Introduction to the Theory of Transformers and Large Language Models, 04.07.2024</p>
    </header>

    <nav>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#modeling">Modeling</a></li>
            <li><a href="#clustering">Clustering</a></li>
            <li><a href="#limitations">Limitations</a></li>
            <li><a href="#future-work">Future Work</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>Transformers, introduced by Vaswani et al. in 2017, revolutionized neural network architectures. The central mechanism is Self-attention. This paper provides a mathematical framework for Self Attention using interacting particle systems, focusing on the clustering behavior of tokens in long-time dynamics.</p>
            <figure>
                <img src="Transformer.png" alt="The Transformer - model architecture">
                <figcaption>The Transformer - model architecture <a href="refs.bib#vaswani2023attentionneed">[Vaswani et al. 2023]</a></figcaption>
            </figure>
        </section>

        <section id="modeling">
            <h2>Modeling</h2>
            <h3>From Self-Attention to Particle Dynamics</h3>
            <p>Self-attention computes a weighted sum of the input tokens to focus on relevant parts of the sequence. For particle \(i\), the attention mechanism assigns weights \(A_{ij}\) to each particle \(j\):</p>
            <pre><code>A_{ij}(t) = \frac{e^{\beta \langle Q x_i(t), K x_j(t) \rangle}}{Z_{\beta, i}(t)}</code></pre>
            <p>where \(Z_{\beta, i}(t)\) is the normalization factor. The update for particle \(i\) based on the attention mechanism is:</p>
            <pre><code>x_i(t+1) = x_i(t) + \sum_{j=1}^n A_{ij}(t) V x_j(t)</code></pre>
            <h3>Deriving the Key Equation</h3>
            <p>We model the evolution of particle \(i\) by projecting the update onto the tangent space at \(x_i(t)\):</p>
            <pre><code>\dot{x}_i(t) = P_{x_i(t)} \left( \frac{1}{Z_{\beta, i}(t)} \sum_{j=1}^n e^{\beta \langle Q x_i(t), K x_j(t) \rangle} V x_j(t) \right)</code></pre>
            <p>Here, \(P_{x_i(t)}\) is the projection operator ensuring the updated position remains on the unit sphere.</p>
        </section>

        <section id="clustering">
            <h2>Clustering</h2>
            <h3>Particle Dynamics Visualization</h3>
            <figure>
                <img src="Figure1_1.png" alt="Histogram of particle dynamics at different layers">
                <figcaption>Histogram of $\left\{ \langle x_i(t), x_j(t) \rangle \right\}_{(i,j) \in [n]^2, i \neq j}$ at different layers t in the context of the pre-trained ALBERT XLarge v2 model. <a href="refs.bib#geshkovski2024mathematicalperspectivetransformers">[Geshkovski et al. 2024]</a></figcaption>
            </figure>

            <h3>Cauchy Problem for Self-Attention</h3>
            <p>We consider the initial positions of particles on the unit sphere \(\mathbb{S}^{d-1}\) and look at simplified self-attention dynamics for $Q,K,V = \mathbf{I}$:</p>
            <pre><code>\dot{x}_i(t) = P_{x_i(t)} \left( \frac{1}{Z_{\beta, i}(t)} \sum_{j=1}^n e^{\beta \langle x_i(t), x_j(t) \rangle} x_j(t) \right)</code></pre>
            <h3>Clustering in High Dimension</h3>
            <p>Main result: In high dimensions (\(d \geq n\)), particles cluster to a single point as \(t \to \infty\).</p>
            <blockquote>
                <p><strong>Theorem 4.1:</strong> Let $n \geq 1$ and $\beta > 0$. Suppose $d \geq n$. Consider the unique solution $(x_i(\cdot))_{i \in [n]} \in C^0(\mathbb{R}_{\geq 0}; (\mathbb{S}^{d-1})^n)$ to the Cauchy problem for (SA). Then almost surely there exists $x^* \in \mathbb{S}^{d-1}$ and constants $C, \lambda > 0$ such that $$\|x_i(t) - x^*\| \leq Ce^{-\lambda t}$$ holds for all $i \in [n]$ and $t \geq 0$.</p>
            </blockquote>
        </section>

        <section id="limitations">
            <h2>Limitations</h2>
            <p>Is this still relevant to a transformer? We left away the linear layers, used projection instead of Layer Normalization, placed restrictions on $Q, K, V$, and used the same matrices for all layers, not considering multiple heads.</p>
        </section>

        <section id="future-work">
            <h2>Future Work</h2>
            <p>Open questions for future research:</p>
            <ul>
                <li>Do the dynamics enter a transient metastable state, where for $\beta \gg 1$, all particles stay in the vicinity of $m < n$ clusters for long periods before collapsing to the final cluster $\{x^*\}$?</li>
                <li>Can the conclusions of the Theorems be generalized to the case of random matrices $Q, K, V$?</li>
            </ul>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>The mathematical framework for Transformers provides insights into clustering behavior, opening questions for future research and bridging the gap between theoretical understanding and practical implementations.</p>
        </section>

        <section id="references">
            <h2>References</h2>
            <ul>
                <li>Vaswani, Ashish, et al. "Attention is All You Need." <em>NeurIPS</em> 2023. <a href="refs.bib#vaswani2023attentionneed">[link]</a></li>
                <li>Geshkovski, Borjan, et al. "A Mathematical Perspective on Transformers." <em>Journal of Machine Learning Research</em> 2024. <a href="refs.bib#geshkovski2024mathematicalperspectivetransformers">[link]</a></li>
            </ul>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Josef Ott. All rights reserved.</p>
    </footer>
</body>
</html>
